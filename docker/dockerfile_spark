# Use the official Bitnami Spark image as a base, which is configured for cluster usage.
FROM bitnami/spark:4.0

# Switch to the root user to have permissions to install packages
USER root

# Install curl for downloading JARs
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

# Copy the requirements file first to leverage Docker's layer caching.
COPY requirements/spark.txt ./requirements.txt

# Install all Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# --- Pre-download Delta Lake + Spark UI JARs ---
RUN mkdir -p /opt/bitnami/spark/delta_jars && \
    cd /opt/bitnami/spark/delta_jars && \
    curl -L -O https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar && \
    curl -L -O https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar && \
    curl -L -O https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.13.1/antlr4-runtime-4.13.1.jar

# Download Spark UI module jar to enable visual graphs
RUN curl -L -o /opt/bitnami/spark/jars/spark-ui_2.13-4.0.0.jar \
    https://repo1.maven.org/maven2/org/apache/spark/spark-ui_2.13/4.0.0/spark-ui_2.13-4.0.0.jar

# Set the HOME directory for the spark user
ENV HOME=/opt/bitnami/spark

# Copy app code and make it importable
COPY ./apps ./apps
COPY log4j2.properties ./apps/log4j2.properties
ENV PYTHONPATH "${PYTHONPATH}:/opt/bitnami/spark/apps"

# Add spark user entry
RUN echo "spark:x:1001:0::/home/spark:/bin/bash" >> /etc/passwd && \
    mkdir -p /home/spark && \
    chown -R 1001:0 /home/spark

# Java opts to simplify authentication
ENV JAVA_TOOL_OPTIONS="-Dhadoop.security.authentication=simple"

# Run script
COPY docker/run_spark.sh ./scripts/run_spark.sh
RUN chmod +x ./scripts/run_spark.sh

# Back to non-root
USER 1001
